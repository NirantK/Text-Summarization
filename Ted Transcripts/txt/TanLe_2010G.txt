now our communication with machines has always been limited to conscious and direct forms whether it's something simple like turning on the lights with a switch
or even as complex as programming robotics we have always had to give a command to a machine or even a series of commands in order for it to do something for us
we observe facial expressions body language and we can intuit feelings and emotions from our dialogue with one another this actually forms a large part of our decision making process
our vision is to introduce this whole new realm of human interaction into human computer interaction so that computers can understand
not only what you direct it to do but it can also respond to your facial expressions and emotional experiences
and what better way to do this than by interpreting the signals naturally produced by our brain our center for control and experience
well it sounds like a pretty good idea but this task as bruno mentioned isn't an easy one for two main reasons first the detection algorithms
our brain is made up of billions of active neurons around one hundred and seventy thousand km of combined axon length when these neurons interact the chemical reaction emits an electrical impulse which can be measured
the majority of our functional brain is distributed over the outer surface layer of the brain and to increase the area that's available for mental capacity the brain surface is highly folded
now this cortical folding presents a significant challenge for interpreting surface electrical impulses each individual 's cortex is folded differently very much like a fingerprint
so even though a signal may come from the same functional part of the brain by the time the structure has been folded its physical location is very different between individuals even identical twins
so that we can map the signals closer to its source and therefore making it capable of working across a mass population
the second challenge is the actual device for observing brainwaves eeg measurements typically involve a hairnet with an array of sensors like the one that you can see here in the photo
a technician will put the electrodes onto the scalp using a conductive gel or paste and usually after a procedure of preparing the scalp by light abrasion
now this is quite time consuming and isn't the most comfortable process and on top of that these systems actually cost in the tens of thousands of dollars so
i'd like to invite onstage evan grant who is one of last year 's speakers who 's kindly agreed to help me to demonstrate what we've been able to develop
so the device that you see is a fourteen channel high fidelity eeg acquisition system it doesn't require any scalp preparation
no conductive gel or paste it only takes a few minutes to put on and for the signals to
it's also wireless so it gives you the freedom to move around and compared to the tens of thousands of dollars for a traditional eeg system this headset only costs a few hundred dollars
now on to the detection algorithms so facial expressions as i mentioned before in emotional experiences are actually designed to work out of the box with some sensitivity adjustments available for
but with the limited time we have available i'd like to show you the cognitive suite which is the ability for you to
now evan is new to this system so what we have to do first is create a new profile for him he 's obviously not joanne so
so the first thing we need to do with the cognitive suite is to start with training a neutral signal
so the idea here now is that evan needs to imagine the object coming forward into the screen and there's a progress bar that will scroll across the screen while he 's doing that
the first time nothing will happen because the system has no idea how he thinks about pull but maintain that thought for the entire duration of the eight seconds so one two three
so once we accept this the cube is live so let 's see if evan can actually try and imagine pulling
so we have a little bit of time available so i'm going to ask evan to do a really difficult task and this one is difficult because it's all about being able to visualize something that doesn't exist in our physical world
this is disappear so what you want at least with movement based actions we do that all the time so you can visualize it but with disappear there's really no analogies
but we can see that it actually works even though you can only hold it for a little bit of time as i said it's a very difficult
to imagine this and the great thing about it is that we've only given the software one instance of how he thinks about disappear as there is a machine learning
there is a leveling system built into this software so that as evan or any user becomes more familiar with the system they can
to add more and more detections so that the system begins to differentiate between different distinct thoughts and once you've trained up the detections
these thoughts can be assigned or mapped to any computing platform application or device so i'd like to show you a few examples because there are many possible applications for this new interface
in games and virtual worlds for example your facial expressions can naturally and intuitively be used to control an avatar or virtual character obviously you can experience the fantasy of magic and control the world with your mind
and also colors lighting sound and effects can dynamically respond to your emotional state to heighten the experience that you're having in real time
on to some applications developed by developers and researchers around the world with robots and simple machines for example
you know from the user interface of the control to
and finally to real life changing applications such as being able to control an electric wheelchair
in this example facial expressions are mapped to the movement now
now we really
we are really only scratching the surface of what is possible today and with the community 's input and also with the involvement of developers and researchers
from around the world we hope you can help us to shape where the technology goes from here thank you so much
